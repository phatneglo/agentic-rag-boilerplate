"""
Base Agent class for all specialized agents.
"""

import asyncio
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from enum import Enum

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from pydantic import BaseModel

from app.core.logging_config import get_logger
from app.core.agent_config import get_openai_config, OPENAI_API_KEY

logger = get_logger(__name__)


class ArtifactType(Enum):
    """Supported artifact types."""
    CODE = "code"
    MERMAID = "mermaid"
    CHART = "chart"
    DOCUMENT = "document"
    ANALYSIS = "analysis"
    HTML = "html"
    JSON = "json"


@dataclass
class AgentResponse:
    """Standard response format for all agents."""
    success: bool
    content: str
    artifacts: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    error: Optional[str] = None


class AgentCapability(BaseModel):
    """Defines what an agent can do."""
    name: str
    description: str
    artifact_types: List[ArtifactType]
    keywords: List[str]
    examples: List[str]


class BaseAgent(ABC):
    """
    Base class for all specialized agents.
    Provides common functionality and interface.
    """
    
    def __init__(self, name: str, capabilities: List[AgentCapability]):
        self.name = name
        self.capabilities = capabilities
        
        # Get OpenAI configuration
        openai_config = get_openai_config()
        
        # Initialize LLM with configuration
        if OPENAI_API_KEY:
            self.llm = ChatOpenAI(
                model=openai_config["model"],
                temperature=openai_config["temperature"],
                max_tokens=openai_config["max_tokens"],
                api_key=openai_config["api_key"],
                streaming=True  # Enable streaming support
            )
        else:
            # Fallback for when API key is not available
            self.llm = None
            
        logger.info(f"Initialized {self.name} agent with {'real' if self.llm else 'mock'} LLM")
    
    @abstractmethod
    def get_system_prompt(self) -> str:
        """Return the system prompt for this agent."""
        pass
    
    @abstractmethod
    async def process_request(self, user_input: str, context: Dict[str, Any] = None, config: Dict[str, Any] = None) -> AgentResponse:
        """Process user request and generate appropriate artifacts."""
        pass
    
    @abstractmethod
    def can_handle(self, user_input: str) -> bool:
        """Determine if this agent can handle the given input."""
        pass
    
    async def generate_response(self, user_input: str, system_prompt: str = None, config: Dict[str, Any] = None) -> str:
        """Generate LLM response using GPT-4o mini with streaming support."""
        try:
            if not self.llm:
                # Mock response when API key is not available
                return self._generate_mock_response(user_input, system_prompt)
            
            messages = [
                SystemMessage(content=system_prompt or self.get_system_prompt()),
                HumanMessage(content=user_input)
            ]
            
            # Check if streaming is requested via config
            if config and config.get("callbacks"):
                logger.info(f"ğŸš€ {self.name}: Starting real-time streaming with callbacks")
                
                # Use astream for real-time token streaming
                full_response = ""
                async for chunk in self.llm.astream(messages, config=config):
                    if hasattr(chunk, 'content') and chunk.content:
                        full_response += chunk.content
                
                logger.info(f"âœ… {self.name}: Completed streaming - {len(full_response)} characters total")
                return full_response
            else:
                # Regular non-streaming request
                response = await self.llm.ainvoke(messages)
                return response.content
            
        except Exception as e:
            logger.error(f"Error generating response in {self.name}: {e}")
            # Fallback to mock response on error
            return self._generate_mock_response(user_input, system_prompt)
    
    def _generate_mock_response(self, user_input: str, system_prompt: str = None) -> str:
        """Generate a mock response when real LLM is not available."""
        logger.warning(f"Using mock response for {self.name} agent")
        
        # Simple mock response based on agent type
        if self.name == "Code":
            return f"""I'll help you with the coding request: "{user_input}"

```python
# Mock code example
def example_function():
    \"\"\"
    This is a mock code response for demonstration.
    In production, this would be generated by GPT-4o mini.
    \"\"\"
    return "Hello from {self.name} Agent!"

# Usage example
result = example_function()
print(result)
```

This is a demonstration response. Please configure OPENAI_API_KEY for full functionality."""
        
        elif self.name == "Diagram":
            return f"""I'll create a diagram for: "{user_input}"

```mermaid
graph TD
    A[User Request] --> B[{self.name} Agent]
    B --> C[Process Request]
    C --> D[Generate Diagram]
    D --> E[Return Artifact]
    
    style A fill:#e1f5fe
    style E fill:#c8e6c9
```

This is a demonstration response. Please configure OPENAI_API_KEY for full functionality."""
        
        else:
            return f"""I understand you're asking about: "{user_input}"

As the {self.name} Agent, I would typically provide a detailed response based on my specialization. However, this is currently a mock response for demonstration purposes.

To get full AI-powered responses, please configure your OPENAI_API_KEY environment variable.

Capabilities of {self.name} Agent:
{self.get_capabilities_summary()}"""
    
    def extract_keywords(self, text: str) -> List[str]:
        """Extract keywords from user input for capability matching."""
        text_lower = text.lower()
        found_keywords = []
        
        for capability in self.capabilities:
            for keyword in capability.keywords:
                if keyword.lower() in text_lower:
                    found_keywords.append(keyword)
        
        return found_keywords
    
    def get_artifact_template(self, artifact_type: ArtifactType) -> Dict[str, Any]:
        """Get template for artifact creation."""
        templates = {
            ArtifactType.CODE: {
                "type": "code",
                "title": "",
                "language": "python",
                "content": "",
                "description": ""
            },
            ArtifactType.MERMAID: {
                "type": "mermaid",
                "title": "",
                "content": "",
                "description": ""
            },
            ArtifactType.CHART: {
                "type": "chart",
                "title": "",
                "chart_type": "line",
                "data": {},
                "options": {}
            },
            ArtifactType.DOCUMENT: {
                "type": "document",
                "title": "",
                "format": "markdown",
                "content": "",
                "sections": []
            },
            ArtifactType.ANALYSIS: {
                "type": "analysis",
                "title": "",
                "summary": "",
                "findings": [],
                "recommendations": []
            },
            ArtifactType.HTML: {
                "type": "html",
                "title": "",
                "content": "",
                "styles": ""
            },
            ArtifactType.JSON: {
                "type": "json",
                "title": "",
                "content": {},
                "schema": {}
            }
        }
        
        return templates.get(artifact_type, {})
    
    def create_artifact(self, artifact_type: ArtifactType, **kwargs) -> Dict[str, Any]:
        """Create an artifact with the given type and parameters."""
        import time
        
        artifact = self.get_artifact_template(artifact_type)
        artifact["id"] = f"artifact_{int(time.time())}_{self.name}"
        artifact.update(kwargs)
        
        return artifact
    
    async def validate_output(self, response: AgentResponse) -> AgentResponse:
        """Validate agent output before returning."""
        if not response.content and not response.artifacts:
            response.success = False
            response.error = "No content or artifacts generated"
        
        return response
    
    def get_capabilities_summary(self) -> str:
        """Get a summary of this agent's capabilities."""
        capability_list = []
        for cap in self.capabilities:
            artifact_types = [t.value for t in cap.artifact_types]
            capability_list.append(f"- {cap.name}: {cap.description} (Creates: {', '.join(artifact_types)})")
        
        return f"{self.name} Agent Capabilities:\n" + "\n".join(capability_list) 